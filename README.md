# AI-Powered-Drone-Classification-System
This project is a final-year Bachelorâ€™s Engineering project focused on classifying aerial objectsâ€”such as birds or dronesâ€”using spectrogram data derived from micro-Doppler radar signatures. The core objective is to train a deep learning model that can distinguish between different object classes based on their spectrograms.

# ğŸ“– Project Summary
This project presents a deep learning system that classifies aerial objectsâ€”specifically birds and dronesâ€”by analyzing motion signals derived from video footage. Using optical flow techniques, we extract 1D motion patterns that reflect the unique movement characteristics of each object type. These patterns are then fed into a 1D Convolutional Neural Network (CNN) to enable accurate classification.

The system supports real-time inference through a Flask-based web interface and has practical use cases in airspace security, wildlife monitoring, and automated surveillance.

# âš™ï¸ Core Components
### ğŸŒ€ Optical Flow Signal Extraction
Frames are extracted from videos using OpenCV.

Farneback's method is used to compute dense optical flow.

The flow vectors are converted into 1D motion signals for downstream analysis.

### ğŸ§ª Dataset Generation
From 161 original video files, we generated a synthetic dataset with 500 samples (250 drone, 250 bird).

All signals are normalized and padded to ensure consistent input size for training.

### ğŸ§  1D CNN Classification Model
A custom 1D Convolutional Neural Network is designed to process temporal motion signals.

The model achieved an overall classification accuracy of 77%.

### ğŸŒ Web-Based Inference
A lightweight Flask web app allows users to upload videos.

The app processes the video, classifies the object, and returns the result along with a visualization of the signal.

### ğŸ§± System Workflow
User uploads a video file via the web interface.

Optical flow signal is computed from the video frames.

The 1D CNN model classifies the motion pattern.

Output is returned to the user with visual feedback.

# ğŸ“ˆ Performance Highlights
âœ… Accuracy: 77% on the synthetic test set

â± Average Inference Time: 20â€“30 seconds per video

ğŸ§  Model Type: 1D CNN (Keras + TensorFlow)

# ğŸ’¡ Use Cases
Restricted Airspace Monitoring â€“ Detect unauthorized drones and distinguish them from birds.

Conservation Research â€“ Monitor bird activity in protected habitats without invasive methods.

Smart Surveillance â€“ Integrate intelligent object classification into camera networks.

# ğŸ”® Future Enhancements
Increase data volume and diversity for better generalization.

Investigate hybrid architectures like CNN-LSTM or Vision Transformers.

Optimize processing for edge deployment (e.g., Jetson Nano, Raspberry Pi).

Introduce multi-class support for additional aerial entities.

Explore fusion with non-visual sensors (e.g., radar, acoustic).

